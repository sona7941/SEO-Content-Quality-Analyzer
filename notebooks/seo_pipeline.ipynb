{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00d5f2fe",
   "metadata": {},
   "source": [
    "# SEO Content Quality & Duplicate Detection\n",
    "\n",
    "ML pipeline for analyzing content quality and finding duplicates.\n",
    "\n",
    "## What this does:\n",
    "\n",
    "1. Parse HTML and extract clean text\n",
    "2. Calculate readability metrics and keywords\n",
    "3. Find duplicate content using TF-IDF similarity\n",
    "4. Train a quality classifier (Low/Medium/High)\n",
    "5. Provide analyze_url() function for testing\n",
    "\n",
    "## Outputs:\n",
    "\n",
    "- `data/extracted_content.csv` - parsed content\n",
    "- `data/features.csv` - full feature set\n",
    "- `data/duplicates.csv` - duplicate pairs (similarity ≥ 0.80)\n",
    "- `models/quality_model.pkl` - trained classifier\n",
    "- `models/tfidf_vectorizer.pkl` - vectorizer\n",
    "- `models/tfidf_matrix.pkl` - document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16572f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\n",
      "Data directory: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\data\n",
      "Models directory: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\models\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "import textstat\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import joblib\n",
    "\n",
    "# download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# setup paths\n",
    "CURRENT_DIR = Path.cwd()\n",
    "if CURRENT_DIR.name == 'notebooks':\n",
    "    ROOT = CURRENT_DIR.parent\n",
    "else:\n",
    "    ROOT = CURRENT_DIR\n",
    "\n",
    "DATA_DIR = ROOT / 'data'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "035aa3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "Loaded 81 rows\n",
      "Columns: ['url', 'html_content']\n",
      "\n",
      "Preview:\n",
      "Loaded 81 rows\n",
      "Columns: ['url', 'html_content']\n",
      "\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>html_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;!--[if lt IE 7]&gt; &lt;html class=\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;\\n    &lt;me...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               url  \\\n",
       "0   https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1  https://www.varonis.com/blog/cybersecurity-tips   \n",
       "\n",
       "                                        html_content  \n",
       "0  <!doctype html><!--[if lt IE 7]> <html class=\"...  \n",
       "1  <!doctype html><html lang=\"en\"><head>\\n    <me...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dataset_path = DATA_DIR / 'data.csv'\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
    "\n",
    "df_raw = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"Loaded {len(df_raw)} rows\")\n",
    "print(f\"Columns: {list(df_raw.columns)}\")\n",
    "print(f\"\\nPreview:\")\n",
    "df_raw.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e8c9b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: HTML PARSING\n",
      "================================================================================\n",
      "Processing HTML content...\n",
      "Extracted content from 81 pages\n",
      "Failed: 16 pages\n",
      "Saved to: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\data\\extracted_content.csv\n",
      "\n",
      "Preview:\n",
      "Extracted content from 81 pages\n",
      "Failed: 16 pages\n",
      "Saved to: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\data\\extracted_content.csv\n",
      "\n",
      "Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>Cyber Security Blog</td>\n",
       "      <td>Cyber Crisis Tabletop Exercise Cyber Security ...</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
       "      <td>Cybersecurity is gaining more importance globa...</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
       "      <td>Cybersecurity is inextricably tied to the tech...</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
       "      <td>Cyberspace is particularly difficult to secure...</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                Cyber Security Blog   \n",
       "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
       "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
       "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
       "4                                                      \n",
       "\n",
       "                                           body_text  word_count  \n",
       "0  Cyber Crisis Tabletop Exercise Cyber Security ...         326  \n",
       "1  Cybersecurity is gaining more importance globa...        1578  \n",
       "2  Cybersecurity is inextricably tied to the tech...         946  \n",
       "3  Cyberspace is particularly difficult to secure...         489  \n",
       "4                                                              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse HTML and extract text\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: HTML PARSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_title_and_body(html: str) -> Tuple[str, str]:\n",
    "    \"\"\"Extract title and body from HTML\"\"\"\n",
    "    if not html or pd.isna(html):\n",
    "        return '', ''\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # get title\n",
    "        title = ''\n",
    "        if soup.title and soup.title.string:\n",
    "            title = soup.title.string.strip()\n",
    "        \n",
    "        # try to get body from article/main tags first\n",
    "        body_text = ''\n",
    "        for tag_name in ['article', 'main']:\n",
    "            element = soup.find(tag_name)\n",
    "            if element:\n",
    "                paragraphs = element.find_all('p')\n",
    "                if paragraphs:\n",
    "                    body_text = ' '.join(p.get_text(separator=' ', strip=True) for p in paragraphs)\n",
    "                    break\n",
    "        \n",
    "        # fallback to all p tags\n",
    "        if not body_text:\n",
    "            paragraphs = soup.find_all('p')\n",
    "            body_text = ' '.join(p.get_text(separator=' ', strip=True) for p in paragraphs)\n",
    "        \n",
    "        # clean whitespace\n",
    "        body_text = re.sub(r'\\s+', ' ', body_text).strip()\n",
    "        \n",
    "        return title, body_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: parse failed - {str(e)[:100]}\")\n",
    "        return '', ''\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return ''\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# process all rows\n",
    "print(\"Processing HTML content...\")\n",
    "parsed_data = []\n",
    "failed_count = 0\n",
    "\n",
    "for idx, row in df_raw.iterrows():\n",
    "    url = row.get('url', '')\n",
    "    html_content = row.get('html_content', '')\n",
    "    \n",
    "    title, body = extract_title_and_body(html_content)\n",
    "    body = clean_text(body)\n",
    "    word_count = len(body.split()) if body else 0\n",
    "    \n",
    "    if not body:\n",
    "        failed_count += 1\n",
    "    \n",
    "    parsed_data.append({\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'body_text': body,\n",
    "        'word_count': word_count\n",
    "    })\n",
    "\n",
    "df_extracted = pd.DataFrame(parsed_data)\n",
    "\n",
    "output_path = DATA_DIR / 'extracted_content.csv'\n",
    "df_extracted.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Extracted content from {len(df_extracted)} pages\")\n",
    "print(f\"Failed: {failed_count} pages\")\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(f\"\\nPreview:\")\n",
    "df_extracted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea6d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "Computing text metrics...\n",
      "Done computing sentence counts and readability\n",
      "\n",
      "Building TF-IDF vectors...\n",
      "TF-IDF matrix shape: (81, 4617)\n",
      "Vocabulary size: 4617\n",
      "\n",
      "Extracting keywords...\n",
      "Extracted top 5 keywords for 81 documents\n",
      "\n",
      "Saving features...\n",
      "Saved features to: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\data\\features.csv\n",
      "Saved TF-IDF vectorizer and matrix\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FEATURE SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "         word_count  sentence_count  flesch_reading_ease\n",
      "count     81.000000       81.000000            81.000000\n",
      "mean    1781.271605       88.827160            33.559259\n",
      "std     3894.245190      203.139717            30.792453\n",
      "min        0.000000        0.000000           -75.370000\n",
      "25%       23.000000        1.000000             0.000000\n",
      "50%      449.000000       22.000000            37.710000\n",
      "75%     1578.000000       88.000000            55.440000\n",
      "max    21386.000000     1404.000000           119.190000\n",
      "\n",
      "Thin content (<500 words): 43 pages (53.1%)\n",
      "Saved features to: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\data\\features.csv\n",
      "Saved TF-IDF vectorizer and matrix\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FEATURE SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "         word_count  sentence_count  flesch_reading_ease\n",
      "count     81.000000       81.000000            81.000000\n",
      "mean    1781.271605       88.827160            33.559259\n",
      "std     3894.245190      203.139717            30.792453\n",
      "min        0.000000        0.000000           -75.370000\n",
      "25%       23.000000        1.000000             0.000000\n",
      "50%      449.000000       22.000000            37.710000\n",
      "75%     1578.000000       88.000000            55.440000\n",
      "max    21386.000000     1404.000000           119.190000\n",
      "\n",
      "Thin content (<500 words): 43 pages (53.1%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>is_thin</th>\n",
       "      <th>top_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>Cyber Security Blog</td>\n",
       "      <td>Cyber Crisis Tabletop Exercise Cyber Security ...</td>\n",
       "      <td>326</td>\n",
       "      <td>6</td>\n",
       "      <td>-18.67</td>\n",
       "      <td>True</td>\n",
       "      <td>cyber|cybersecurity|training|events|clients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
       "      <td>Cybersecurity is gaining more importance globa...</td>\n",
       "      <td>1578</td>\n",
       "      <td>78</td>\n",
       "      <td>41.50</td>\n",
       "      <td>False</td>\n",
       "      <td>access|data|security|app|sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
       "      <td>Cybersecurity is inextricably tied to the tech...</td>\n",
       "      <td>946</td>\n",
       "      <td>61</td>\n",
       "      <td>55.44</td>\n",
       "      <td>False</td>\n",
       "      <td>password|protect|authentication|device|use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
       "      <td>Cyberspace is particularly difficult to secure...</td>\n",
       "      <td>489</td>\n",
       "      <td>22</td>\n",
       "      <td>15.10</td>\n",
       "      <td>True</td>\n",
       "      <td>cybersecurity|cyber|nation|offers|resilience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                Cyber Security Blog   \n",
       "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
       "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
       "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
       "4                                                      \n",
       "\n",
       "                                           body_text  word_count  \\\n",
       "0  Cyber Crisis Tabletop Exercise Cyber Security ...         326   \n",
       "1  Cybersecurity is gaining more importance globa...        1578   \n",
       "2  Cybersecurity is inextricably tied to the tech...         946   \n",
       "3  Cyberspace is particularly difficult to secure...         489   \n",
       "4                                                              0   \n",
       "\n",
       "   sentence_count  flesch_reading_ease  is_thin  \\\n",
       "0               6               -18.67     True   \n",
       "1              78                41.50    False   \n",
       "2              61                55.44    False   \n",
       "3              22                15.10     True   \n",
       "4               0                 0.00     True   \n",
       "\n",
       "                                   top_keywords  \n",
       "0   cyber|cybersecurity|training|events|clients  \n",
       "1            access|data|security|app|sensitive  \n",
       "2    password|protect|authentication|device|use  \n",
       "3  cybersecurity|cyber|nation|offers|resilience  \n",
       "4                                                "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature engineering\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_features = df_extracted.copy()\n",
    "\n",
    "# Basic text metrics\n",
    "print(\"\\nComputing text metrics...\")\n",
    "\n",
    "def safe_sentence_count(text: str) -> int:\n",
    "    if not text or pd.isna(text):\n",
    "        return 0\n",
    "    try:\n",
    "        return len(sent_tokenize(text))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def safe_readability_score(text: str) -> float:\n",
    "    if not text or pd.isna(text) or len(text.split()) == 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        return textstat.flesch_reading_ease(text)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "df_features['sentence_count'] = df_features['body_text'].apply(safe_sentence_count)\n",
    "df_features['flesch_reading_ease'] = df_features['body_text'].apply(safe_readability_score)\n",
    "df_features['is_thin'] = df_features['word_count'] < 500\n",
    "\n",
    "print(f\"Done computing sentence counts and readability\")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "print(\"\\nBuilding TF-IDF vectors...\")\n",
    "\n",
    "corpus = df_features['body_text'].fillna('').tolist()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    max_df=0.95,\n",
    "    min_df=2\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Extract top keywords\n",
    "print(\"\\nExtracting keywords...\")\n",
    "\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "def extract_top_keywords(doc_vector, n: int = 5) -> str:\n",
    "    if doc_vector.nnz == 0:\n",
    "        return ''\n",
    "    \n",
    "    values = doc_vector.toarray().ravel()\n",
    "    top_indices = values.argsort()[::-1][:n]\n",
    "    keywords = feature_names[top_indices]\n",
    "    \n",
    "    return '|'.join(keywords)\n",
    "\n",
    "top_keywords_list = []\n",
    "for i in range(tfidf_matrix.shape[0]):\n",
    "    keywords = extract_top_keywords(tfidf_matrix[i], n=5)\n",
    "    top_keywords_list.append(keywords)\n",
    "\n",
    "df_features['top_keywords'] = top_keywords_list\n",
    "\n",
    "print(f\"Extracted top 5 keywords for {len(df_features)} documents\")\n",
    "\n",
    "# Save everything\n",
    "print(\"\\nSaving features...\")\n",
    "\n",
    "df_features_output = df_features[[\n",
    "    'url', 'title', 'body_text', 'word_count', \n",
    "    'sentence_count', 'flesch_reading_ease', 'is_thin', 'top_keywords'\n",
    "]].copy()\n",
    "\n",
    "features_path = DATA_DIR / 'features.csv'\n",
    "df_features_output.to_csv(features_path, index=False)\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, MODELS_DIR / 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(tfidf_matrix, MODELS_DIR / 'tfidf_matrix.pkl')\n",
    "\n",
    "print(f\"Saved features to: {features_path}\")\n",
    "print(f\"Saved TF-IDF vectorizer and matrix\")\n",
    "\n",
    "# Summary stats\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"FEATURE SUMMARY\")\n",
    "print(\"-\"*80)\n",
    "print(df_features[['word_count', 'sentence_count', 'flesch_reading_ease']].describe())\n",
    "print(f\"\\nThin content (<500 words): {df_features['is_thin'].sum()} pages ({df_features['is_thin'].mean()*100:.1f}%)\")\n",
    "\n",
    "df_features_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779f4522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: FINDING DUPLICATES\n",
      "================================================================================\n",
      "\n",
      "Computing similarities (threshold: 0.8)...\n",
      "\n",
      "Analyzed 81 pages\n",
      "Found 0 duplicate pairs (>=0.8)\n",
      "Saved to: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\data\\duplicates.csv\n",
      "\n",
      "No duplicates found at this threshold\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate detection\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: FINDING DUPLICATES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.80\n",
    "\n",
    "print(f\"\\nComputing similarities (threshold: {SIMILARITY_THRESHOLD})...\")\n",
    "\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# find duplicate pairs\n",
    "duplicate_pairs = []\n",
    "n_docs = similarity_matrix.shape[0]\n",
    "\n",
    "for i in range(n_docs):\n",
    "    for j in range(i + 1, n_docs):\n",
    "        sim_score = similarity_matrix[i, j]\n",
    "        \n",
    "        if sim_score >= SIMILARITY_THRESHOLD:\n",
    "            duplicate_pairs.append({\n",
    "                'url1': df_features.loc[i, 'url'],\n",
    "                'url2': df_features.loc[j, 'url'],\n",
    "                'similarity': round(float(sim_score), 4)\n",
    "            })\n",
    "\n",
    "df_duplicates = pd.DataFrame(duplicate_pairs)\n",
    "\n",
    "duplicates_path = DATA_DIR / 'duplicates.csv'\n",
    "df_duplicates.to_csv(duplicates_path, index=False)\n",
    "\n",
    "print(f\"\\nAnalyzed {n_docs} pages\")\n",
    "print(f\"Found {len(df_duplicates)} duplicate pairs (>={SIMILARITY_THRESHOLD})\")\n",
    "print(f\"Saved to: {duplicates_path}\")\n",
    "\n",
    "if len(df_duplicates) > 0:\n",
    "    print(f\"\\nTop 5 most similar pairs:\")\n",
    "    print(df_duplicates.nlargest(5, 'similarity'))\n",
    "else:\n",
    "    print(\"\\nNo duplicates found at this threshold\")\n",
    "\n",
    "df_duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74fd3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: QUALITY CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "Generating quality labels...\n",
      "\n",
      "Label distribution:\n",
      "  Low: 50 (61.7%)\n",
      "  Medium: 24 (29.6%)\n",
      "  High: 7 (8.6%)\n",
      "\n",
      "Preparing feature matrix...\n",
      "Feature matrix: (81, 53)\n",
      "Using 3 basic features + 50 TF-IDF features = 53 total\n",
      "\n",
      "Splitting data...\n",
      "Train: 56 samples\n",
      "Test: 25 samples\n",
      "\n",
      "Training RandomForest...\n",
      "Model trained\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE\n",
      "================================================================================\n",
      "Accuracy: 0.9200\n",
      "F1-Score: 0.9177\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      0.50      0.67         2\n",
      "         Low       1.00      0.94      0.97        16\n",
      "      Medium       0.78      1.00      0.88         7\n",
      "\n",
      "    accuracy                           0.92        25\n",
      "   macro avg       0.93      0.81      0.84        25\n",
      "weighted avg       0.94      0.92      0.92        25\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1  0  1]\n",
      " [ 0 15  1]\n",
      " [ 0  0  7]]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "BASELINE (word count only)\n",
      "--------------------------------------------------------------------------------\n",
      "Baseline Accuracy: 0.8000\n",
      "Improvement: +12.00%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TOP 10 FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "            feature  importance\n",
      "         word_count    0.296808\n",
      "     sentence_count    0.287323\n",
      "flesch_reading_ease    0.153482\n",
      "           tfidf_13    0.041952\n",
      "            tfidf_0    0.041276\n",
      "            tfidf_2    0.038521\n",
      "           tfidf_40    0.028503\n",
      "           tfidf_21    0.018673\n",
      "           tfidf_29    0.018665\n",
      "           tfidf_17    0.009358\n",
      "\n",
      "Saving model...\n",
      "            feature  importance\n",
      "         word_count    0.296808\n",
      "     sentence_count    0.287323\n",
      "flesch_reading_ease    0.153482\n",
      "           tfidf_13    0.041952\n",
      "            tfidf_0    0.041276\n",
      "            tfidf_2    0.038521\n",
      "           tfidf_40    0.028503\n",
      "           tfidf_21    0.018673\n",
      "           tfidf_29    0.018665\n",
      "           tfidf_17    0.009358\n",
      "\n",
      "Saving model...\n",
      "Saved to: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\models\\quality_model.pkl\n",
      "Saved to: c:\\Users\\sonam\\OneDrive\\Desktop\\leadwalnut\\models\\quality_model.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word_count</td>\n",
       "      <td>0.296808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence_count</td>\n",
       "      <td>0.287323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flesch_reading_ease</td>\n",
       "      <td>0.153482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tfidf_13</td>\n",
       "      <td>0.041952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tfidf_0</td>\n",
       "      <td>0.041276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tfidf_2</td>\n",
       "      <td>0.038521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tfidf_40</td>\n",
       "      <td>0.028503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tfidf_21</td>\n",
       "      <td>0.018673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tfidf_29</td>\n",
       "      <td>0.018665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tfidf_17</td>\n",
       "      <td>0.009358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                feature  importance\n",
       "0            word_count    0.296808\n",
       "1        sentence_count    0.287323\n",
       "2   flesch_reading_ease    0.153482\n",
       "16             tfidf_13    0.041952\n",
       "3               tfidf_0    0.041276\n",
       "5               tfidf_2    0.038521\n",
       "43             tfidf_40    0.028503\n",
       "24             tfidf_21    0.018673\n",
       "32             tfidf_29    0.018665\n",
       "20             tfidf_17    0.009358"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train quality classifier\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: QUALITY CLASSIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate labels\n",
    "print(\"\\nGenerating quality labels...\")\n",
    "\n",
    "def assign_quality_label(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Label quality based on word count and readability\n",
    "    High: >1500 words AND readability 50-70\n",
    "    Low: <500 words OR readability <30\n",
    "    Medium: everything else\n",
    "    \"\"\"\n",
    "    word_count = row['word_count']\n",
    "    readability = row['flesch_reading_ease']\n",
    "    \n",
    "    if word_count > 1500 and 50 <= readability <= 70:\n",
    "        return 'High'\n",
    "    elif word_count < 500 or readability < 30:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "df_model = df_features.copy()\n",
    "df_model['quality_label'] = df_model.apply(assign_quality_label, axis=1)\n",
    "\n",
    "label_counts = df_model['quality_label'].value_counts()\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label}: {count} ({count/len(df_model)*100:.1f}%)\")\n",
    "\n",
    "# Prepare features\n",
    "print(\"\\nPreparing feature matrix...\")\n",
    "\n",
    "N_TFIDF_FEATURES = 50\n",
    "n_tfidf = min(N_TFIDF_FEATURES, tfidf_matrix.shape[1])\n",
    "\n",
    "X_tfidf = tfidf_matrix[:, :n_tfidf].toarray()\n",
    "X_basic = df_model[['word_count', 'sentence_count', 'flesch_reading_ease']].fillna(0).values\n",
    "\n",
    "X = np.hstack([X_basic, X_tfidf])\n",
    "y = df_model['quality_label'].values\n",
    "\n",
    "print(f\"Feature matrix: {X.shape}\")\n",
    "print(f\"Using 3 basic features + {n_tfidf} TF-IDF features = {X.shape[1]} total\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\nSplitting data...\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining RandomForest...\")\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Model trained\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating...\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PERFORMANCE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Baseline comparison\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"BASELINE (word count only)\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def baseline_predict(X_features: np.ndarray) -> np.ndarray:\n",
    "    predictions = []\n",
    "    for word_count in X_features[:, 0]:\n",
    "        if word_count > 1500:\n",
    "            predictions.append('High')\n",
    "        elif word_count < 500:\n",
    "            predictions.append('Low')\n",
    "        else:\n",
    "            predictions.append('Medium')\n",
    "    return np.array(predictions)\n",
    "\n",
    "y_baseline = baseline_predict(X_test)\n",
    "baseline_accuracy = accuracy_score(y_test, y_baseline)\n",
    "\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"Improvement: {(accuracy - baseline_accuracy)*100:+.2f}%\")\n",
    "\n",
    "# Feature importance\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TOP 10 FEATURES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "feature_names_list = ['word_count', 'sentence_count', 'flesch_reading_ease'] + \\\n",
    "                     [f'tfidf_{i}' for i in range(n_tfidf)]\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names_list,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Save model\n",
    "print(\"\\nSaving model...\")\n",
    "\n",
    "model_path = MODELS_DIR / 'quality_model.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "print(f\"Saved to: {model_path}\")\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfce7669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: ANALYSIS FUNCTION\n",
      "================================================================================\n",
      "\n",
      "Function ready: analyze_url(url)\n",
      "\n",
      "Example:\n",
      "  result = analyze_url('https://example.com/article')\n",
      "  print(json.dumps(result, indent=2))\n"
     ]
    }
   ],
   "source": [
    "# Real-time analysis function\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: ANALYSIS FUNCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def scrape_url(url: str, timeout: int = 10) -> str:\n",
    "    \"\"\"Scrape HTML from URL\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error: {str(e)[:100]}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "def analyze_url(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze a URL for quality and duplicates\n",
    "    \n",
    "    Steps:\n",
    "    1. Scrape the URL\n",
    "    2. Extract text\n",
    "    3. Compute features\n",
    "    4. Predict quality\n",
    "    5. Find similar pages\n",
    "    \"\"\"\n",
    "    # load models\n",
    "    vectorizer = joblib.load(MODELS_DIR / 'tfidf_vectorizer.pkl')\n",
    "    doc_matrix = joblib.load(MODELS_DIR / 'tfidf_matrix.pkl')\n",
    "    classifier = joblib.load(MODELS_DIR / 'quality_model.pkl')\n",
    "    \n",
    "    # scrape\n",
    "    html = scrape_url(url)\n",
    "    if not html:\n",
    "        return {\n",
    "            'url': url,\n",
    "            'error': 'Failed to scrape URL',\n",
    "            'quality_label': None\n",
    "        }\n",
    "    \n",
    "    # extract\n",
    "    title, body = extract_title_and_body(html)\n",
    "    body = clean_text(body)\n",
    "    \n",
    "    # features\n",
    "    word_count = len(body.split()) if body else 0\n",
    "    sentence_count = safe_sentence_count(body)\n",
    "    readability = safe_readability_score(body)\n",
    "    is_thin = word_count < 500\n",
    "    \n",
    "    # vectorize\n",
    "    tfidf_vector = vectorizer.transform([body])\n",
    "    \n",
    "    # find similar\n",
    "    similarities = cosine_similarity(tfidf_vector, doc_matrix).ravel()\n",
    "    top_indices = similarities.argsort()[::-1][:5]\n",
    "    \n",
    "    similar_pages = []\n",
    "    for idx in top_indices:\n",
    "        sim_score = similarities[idx]\n",
    "        if sim_score > 0.4:\n",
    "            similar_pages.append({\n",
    "                'url': df_extracted.loc[idx, 'url'],\n",
    "                'title': df_extracted.loc[idx, 'title'],\n",
    "                'similarity': round(float(sim_score), 4)\n",
    "            })\n",
    "    \n",
    "    # predict\n",
    "    n_tfidf = min(50, tfidf_vector.shape[1])\n",
    "    X_tfidf_features = tfidf_vector[:, :n_tfidf].toarray()\n",
    "    X_basic_features = np.array([[word_count, sentence_count, readability]])\n",
    "    X_combined = np.hstack([X_basic_features, X_tfidf_features])\n",
    "    \n",
    "    quality_label = classifier.predict(X_combined)[0]\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'title': title,\n",
    "        'word_count': int(word_count),\n",
    "        'sentence_count': int(sentence_count),\n",
    "        'flesch_reading_ease': round(float(readability), 2),\n",
    "        'is_thin_content': bool(is_thin),\n",
    "        'quality_label': quality_label,\n",
    "        'similar_pages': similar_pages[:3]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"\\nFunction ready: analyze_url(url)\")\n",
    "print(\"\\nExample:\")\n",
    "print(\"  result = analyze_url('https://example.com/article')\")\n",
    "print(\"  print(json.dumps(result, indent=2))\")\n",
    "\n",
    "# Uncomment to test:\n",
    "# test_url = df_extracted.loc[0, 'url']\n",
    "# result = analyze_url(test_url)\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af0297",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "All steps completed successfully.\n",
    "\n",
    "### Generated files:\n",
    "\n",
    "**Data:**\n",
    "- `data/extracted_content.csv` - parsed HTML content\n",
    "- `data/features.csv` - all features with keywords\n",
    "- `data/duplicates.csv` - duplicate pairs (similarity ≥ 0.80)\n",
    "\n",
    "**Models:**\n",
    "- `models/quality_model.pkl` - trained RandomForest\n",
    "- `models/tfidf_vectorizer.pkl` - TF-IDF vectorizer\n",
    "- `models/tfidf_matrix.pkl` - document vectors\n",
    "\n",
    "### Usage:\n",
    "\n",
    "```python\n",
    "# Analyze new URL\n",
    "result = analyze_url('https://example.com/article')\n",
    "print(json.dumps(result, indent=2))\n",
    "\n",
    "# Load model for batch processing\n",
    "model = joblib.load('models/quality_model.pkl')\n",
    "vectorizer = joblib.load('models/tfidf_vectorizer.pkl')\n",
    "```\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- HTML parsing prioritizes `<article>` and `<main>` tags\n",
    "- Using TF-IDF (top 50 dims) + 3 basic features\n",
    "- Similarity threshold at 0.80 to avoid false positives\n",
    "- RandomForest chosen for mixed feature types\n",
    "- Labels are synthetic (based on word count + readability)\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- TF-IDF won't catch paraphrases (would need embeddings)\n",
    "- Readability can be wonky on short text\n",
    "- Synthetic labels may not match real quality\n",
    "- Need rate limiting for production scraping\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- Try sentence-transformers for better similarity\n",
    "- Add visualizations (word clouds, heatmaps)\n",
    "- Build Streamlit UI for easier testing\n",
    "- Add more NLP features (sentiment, entities, topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
